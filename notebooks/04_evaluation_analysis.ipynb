{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c8497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from config import CONFIG\n",
    "from src.evaluation.metrics import precision_at_k, recall_at_k, ndcg_at_k, hit_rate, rmse\n",
    "\n",
    "# %%\n",
    "# Load test data and models\n",
    "print(\"Loading data and models...\")\n",
    "test_data = pd.read_csv('../data/processed/test.csv')\n",
    "user_features = pd.read_csv('../data/processed/user_features.csv')\n",
    "item_features = pd.read_csv('../data/processed/item_features.csv')\n",
    "\n",
    "with open('../experiments/results/trained_models.pkl', 'rb') as f:\n",
    "    models = pickle.load(f)\n",
    "\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Models loaded: {list(models.keys())}\")\n",
    "\n",
    "# %%\n",
    "# Evaluate all models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATING ALL MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "K = 10\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    if name == 'hybrid_nn':\n",
    "        predictions = model.predict(test_data, user_features, item_features)\n",
    "    else:\n",
    "        predictions = model.predict(test_data)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'Precision@10': precision_at_k(test_data, predictions, K),\n",
    "        'Recall@10': recall_at_k(test_data, predictions, K),\n",
    "        'NDCG@10': ndcg_at_k(test_data, predictions, K),\n",
    "        'Hit Rate': hit_rate(test_data, predictions, K),\n",
    "        'RMSE': rmse(test_data['rating'].values, predictions)\n",
    "    }\n",
    "    \n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# %%\n",
    "# Cold-start user evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COLD-START USER EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cold_test = test_data[test_data['is_cold_user'] == True]\n",
    "warm_test = test_data[test_data['is_cold_user'] == False]\n",
    "\n",
    "cold_results = {}\n",
    "for name, model in models.items():\n",
    "    if name == 'hybrid_nn':\n",
    "        cold_pred = model.predict(cold_test, user_features, item_features)\n",
    "    else:\n",
    "        cold_pred = model.predict(cold_test)\n",
    "    \n",
    "    cold_results[name] = {\n",
    "        'Cold Precision@10': precision_at_k(cold_test, cold_pred, K),\n",
    "        'Cold Hit Rate': hit_rate(cold_test, cold_pred, K)\n",
    "    }\n",
    "\n",
    "cold_df = pd.DataFrame(cold_results).T\n",
    "print(cold_df.round(4))\n",
    "\n",
    "# %%\n",
    "# Hypothesis Testing\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HYPOTHESIS TESTING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# H1: Hybrid NN vs Standard NCF\n",
    "hybrid_prec = results['hybrid_nn']['Precision@10']\n",
    "ncf_prec = results['ncf']['Precision@10']\n",
    "diff = hybrid_prec - ncf_prec\n",
    "\n",
    "print(f\"\\nH1: Hybrid NN Precision@10 > NCF Precision@10\")\n",
    "print(f\"Hybrid NN: {hybrid_prec:.4f}\")\n",
    "print(f\"NCF: {ncf_prec:.4f}\")\n",
    "print(f\"Difference: {diff:.4f}\")\n",
    "\n",
    "if diff > 0.03:\n",
    "    print(\"✓ HYPOTHESIS SUPPORTED: Difference > 0.03\")\n",
    "else:\n",
    "    print(\"✗ HYPOTHESIS NOT SUPPORTED: Difference <= 0.03\")\n",
    "\n",
    "# %%\n",
    "# Check all sub-hypotheses\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUB-HYPOTHESIS VERIFICATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "hypotheses = {\n",
    "    'H1.1: Hit Rate >= 0.70': cold_df.loc['hybrid_nn', 'Cold Hit Rate'] >= 0.70,\n",
    "    'H1.2: NDCG improvement >= 8%': (results['hybrid_nn']['NDCG@10'] - results['ncf']['NDCG@10']) / results['ncf']['NDCG@10'] >= 0.08,\n",
    "    'H1.3: Cold user RMSE < 0.90': results['hybrid_nn']['RMSE'] < 0.90,\n",
    "    'H1.4: Precision@10 >= 0.35': results['hybrid_nn']['Precision@10'] >= 0.35\n",
    "}\n",
    "\n",
    "for h, passed in hypotheses.items():\n",
    "    status = \"✓ PASS\" if passed else \"✗ FAIL\"\n",
    "    print(f\"{status}: {h}\")\n",
    "\n",
    "passed_count = sum(hypotheses.values())\n",
    "print(f\"\\nTotal: {passed_count}/4 hypotheses passed\")\n",
    "\n",
    "# %%\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Overall performance comparison\n",
    "ax1 = axes[0, 0]\n",
    "metrics = ['Precision@10', 'Recall@10', 'NDCG@10', 'Hit Rate']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.15\n",
    "\n",
    "for i, (name, vals) in enumerate(results.items()):\n",
    "    ax1.bar(x + i*width, [vals[m] for m in metrics], width, label=name)\n",
    "\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Model Performance Comparison')\n",
    "ax1.set_xticks(x + width*2)\n",
    "ax1.set_xticklabels(metrics, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.axhline(y=0.35, color='r', linestyle='--', label='Target')\n",
    "\n",
    "# 2. Cold-start performance\n",
    "ax2 = axes[0, 1]\n",
    "cold_df.plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('Cold-Start User Performance')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.axhline(y=0.70, color='r', linestyle='--')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# 3. RMSE comparison\n",
    "ax3 = axes[1, 0]\n",
    "rmse_values = [results[m]['RMSE'] for m in results.keys()]\n",
    "colors = ['green' if v < 0.90 else 'red' for v in rmse_values]\n",
    "ax3.bar(results.keys(), rmse_values, color=colors)\n",
    "ax3.set_title('RMSE Comparison')\n",
    "ax3.set_ylabel('RMSE')\n",
    "ax3.axhline(y=0.90, color='r', linestyle='--', label='Target')\n",
    "\n",
    "# 4. Improvement over baselines\n",
    "ax4 = axes[1, 1]\n",
    "baseline = results['random']['Precision@10']\n",
    "improvements = [(results[m]['Precision@10'] - baseline) / baseline * 100 for m in results.keys()]\n",
    "ax4.bar(results.keys(), improvements)\n",
    "ax4.set_title('Improvement over Random Baseline (%)')\n",
    "ax4.set_ylabel('Improvement %')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../experiments/results/evaluation_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Save final results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Combine all results\n",
    "final_results = {\n",
    "    'overall': results_df,\n",
    "    'cold_start': cold_df,\n",
    "    'hypotheses': hypotheses\n",
    "}\n",
    "\n",
    "results_df.to_csv('../experiments/results/final_results.csv')\n",
    "cold_df.to_csv('../experiments/results/cold_start_results.csv')\n",
    "\n",
    "with open('../experiments/results/hypothesis_results.pkl', 'wb') as f:\n",
    "    pickle.dump(hypotheses, f)\n",
    "\n",
    "print(\"✓ Results saved!\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
